

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/icon.jpeg">
  <link rel="icon" href="/images/icon.jpeg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Mehrdad Karami">
  <meta name="keywords" content="Java,SpringBoot, Kafka, Kafka Streams, Software Architecture, Best code practices">
  
    <meta name="description" content="Processing continuous data streams in distributed systems without any time delay poses a number of challenges. We show you how stream processing can succeed with Kafka Streams and Spring Boot.  Everyt">
<meta property="og:type" content="article">
<meta property="og:title" content="Spring Boot and Kafka Stream, Processing continuous data streams">
<meta property="og:url" content="https://java-springboot-kafka.github.io/2022/11/13/kafka-streams-and-spring-boot/index.html">
<meta property="og:site_name" content="Java SpringBoot Kafka Best code practices">
<meta property="og:description" content="Processing continuous data streams in distributed systems without any time delay poses a number of challenges. We show you how stream processing can succeed with Kafka Streams and Spring Boot.  Everyt">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://media.graphassets.com/S9gkDSDBR8mtW0N24a95">
<meta property="og:image" content="https://java-springboot-kafka.github.io/images/Xy3kzvYgT6GRsaxYg6va.png">
<meta property="og:image" content="https://java-springboot-kafka.github.io/images/K3ygZt4RvWW2Evm17u2w.png">
<meta property="article:published_time" content="2022-11-13T17:46:23.071Z">
<meta property="article:modified_time" content="2022-11-13T17:46:23.071Z">
<meta property="article:author" content="Mehrdad Karami">
<meta property="article:tag" content="Java,SpringBoot, Kafka, Kafka Streams, Software Architecture, Best code practices">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://media.graphassets.com/S9gkDSDBR8mtW0N24a95">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Spring Boot and Kafka Stream, Processing continuous data streams - Java SpringBoot Kafka Best code practices</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"java-springboot-kafka.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"Java"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/images/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Mehrdad Karami</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/images/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Spring Boot and Kafka Stream, Processing continuous data streams"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Mehrdad Karami
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-11-13 18:46" pubdate>
          November 13, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          27k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          224 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Spring Boot and Kafka Stream, Processing continuous data streams</h1>
            
            
              <div class="markdown-body">
                
                <p>Processing continuous data streams in distributed systems without any time delay poses a number of challenges. We show you how stream processing can succeed with Kafka Streams and Spring Boot.</p>
<p><img src="https://media.graphassets.com/S9gkDSDBR8mtW0N24a95" srcset="/images/loading.gif" lazyload></p>
<p>Everything in flow: If you look at data as a continuous stream of information, you can get a lot of speed out of it.</p>
<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>Here’s the ultra-short summary:</p>
<ul>
<li>Stream processing is well suited to process large amounts of data asynchronously and with minimal delay.</li>
<li>Modern streaming frameworks allow you to align your application architecture completely with event streams and turn your data management inside out. The event stream becomes the “ <em>source of truth</em> ”.</li>
<li>With Kafka Streams, Kafka offers an API to process streams and map complex operations to them. means<a target="_blank" rel="noopener" href="https://docs.confluent.io/platform/current/streams/concepts.html#streams-concepts-kstream"><em>KStreams</em></a> and<a target="_blank" rel="noopener" href="https://docs.confluent.io/platform/current/streams/concepts.html#ktable"><em>KTables</em></a> you can also map more complex use cases that have to maintain a state. This state is managed by Kafka, so you don’t have to worry about data management yourself.</li>
<li>Spring Boot offers one<a target="_blank" rel="noopener" href="https://spring.io/projects/spring-cloud-stream">Stream abstraction</a> that can be used to implement stream processing workloads.</li>
</ul>
<p>You can find the entire project at<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry">GitHub</a>.</p>
<h2 id="Processing-large-amounts-of-data-quickly-–-a-perennial-topic"><a href="#Processing-large-amounts-of-data-quickly-–-a-perennial-topic" class="headerlink" title="Processing large amounts of data quickly – a perennial topic"></a>Processing large amounts of data quickly – a perennial topic</h2><p>In our everyday project environment, we often deal with use cases in which we have to process a continuous stream of events through several systems involved with as little delay as possible. Two examples:</p>
<ul>
<li>Let’s imagine a classic web shop: customers order goods around the clock. The information about the incoming order is of interest for various subsystems: Among other things, the warehouse needs information about the items to be shipped, we have to write an invoice and maybe now reorder goods ourselves.</li>
<li>Another scenario: A car manufacturer analyzes their vehicles’ telemetry data to improve the durability of their vehicles. For this purpose, the components of thousands of cars send sensor data every second, which then has to be examined for anomalies.</li>
</ul>
<p>The larger the amounts of data in both examples, the more difficult it becomes for us to scale our system adequately and to process the data in the shortest possible time. This describes a general problem: the volume of data that we are confronted with in everyday life is constantly increasing, while our customers expect us to process the data and make it usable as quickly as possible.<a href="">Modern Stream Processing Frameworks</a> should address precisely these aspects.</p>
<p>In this blog post, we would like to use a concrete use case to demonstrate how a stream processing architecture can be implemented with Spring Boot and Apache Kafka Streams. We want to go into the conception of the overall system as well as the everyday problems that we should take into account during implementation.</p>
<h2 id="Streams-and-events-briefly-outlined"><a href="#Streams-and-events-briefly-outlined" class="headerlink" title="Streams and events briefly outlined"></a>Streams and events briefly outlined</h2><p>We can classify stream processing as an alternative to batch processing. Instead of “heaping” all incoming data and processing them en bloc at a later point in time, the idea behind stream processing is to view incoming data as a continuous stream: the data is processed continuously. Depending on the API and programming model, we can use a corresponding domain-specific language to define operations on the data stream. Sender and receiver do not need any knowledge about each other. The systems participating in the stream are usually decoupled from one another via a corresponding messaging backbone.</p>
<p>In addition to the advantages of time-critical processing of data, the concept is well suited to reducing dependencies between services in distributed systems. Through indirection via a central messaging backbone, services can now switch to an asynchronous communication model in that they no longer communicate via commands but via events. While commands are direct, synchronous calls between services that trigger an action or result in a change of state, events only transmit the information that an event has occurred. With event processing, the recipient decides when and how this information is processed. This procedure is helpful to achieve a looser coupling between the components of an overall system<a target="_blank" rel="noopener" href="https://www.confluent.io/designing-event-driven-systems/">[1]</a>.</p>
<p>Using Kafka as a stream processing platform allows us to align our overall system to events, as the next section shows.</p>
<h2 id="Kafka-as-a-stream-processing-platform"><a href="#Kafka-as-a-stream-processing-platform" class="headerlink" title="Kafka as a stream processing platform"></a>Kafka as a stream processing platform</h2><p>What distinguishes Kafka from classic message brokers such as RabbitMQ or Amazon SQS is the permanent storage of event streams and the provision of an API for processing these events as streams. This enables us to turn the architecture of a distributed system inside out and make these events the <em>“source of truth”</em> : If the state of our entire system can be established based on the sequence of all events and these events are stored permanently, this state can be changed at any time by processing of the event log can be (re)established. Martin Kleppmann described the concept of a globally available, unchangeable event log as <em>“turning the database inside out”</em> .<a target="_blank" rel="noopener" href="https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/">[2]</a>. What is meant by this is that we can distribute the concepts that we traditionally provide encapsulated as a black box within a relational database (a transaction log, a query engine, indexes and caches) through Kafka and Kafka Streams to the components of a system.<br>To build a streaming architecture based on this theory, we use two different components from the Kafka ecosystem:</p>
<ul>
<li><strong>Kafka Cluster</strong> : Provides event storage. Acts as the immutable and permanently stored transaction log.</li>
<li><strong>Kafka Streams</strong> : Provides the API for stream processing (Streams API). Abstracts the components for generating and consuming the messages and provides the programming model to process the events and map caches and queries to them<a target="_blank" rel="noopener" href="https://kafka.apache.org/30/documentation/streams/core-concepts">[3]</a></li>
</ul>
<p>In addition to aligning to events and providing APIs to process them, Kafka also comes with some mechanisms to scale with large amounts of data. The most important mechanism is partitioning: messages are distributed to different partitions so that they can be read and written in parallel as efficiently as possible. Provides a good overview of the central concepts and vocabulary in the Kafka cosmos<a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/#intro_concepts_and_terms">[4]</a>.</p>
<p>Based on a concrete use case, we now want to show you how we can implement a distributed streaming architecture with Spring Boot and Kafka.</p>
<h2 id="An-exemplary-use-case"><a href="#An-exemplary-use-case" class="headerlink" title="An exemplary use case"></a>An exemplary use case</h2><p>Let’s imagine that our customer - a space agency - commissioned us to develop a system for evaluating telemetry data from various space probes in space. The general conditions and requirements are as follows:</p>
<ul>
<li>We have an unspecified set of probes giving us a steady stream of telemetry readings. These probes belong to either the US Space Agency (NASA) or the European Space Agency (ESA).</li>
<li>All space probes send their measurement data in the imperial system.</li>
<li>Our customer is only interested in the aggregated measurement data per probe:<ul>
<li>What is the total distance a given probe has traveled so far?</li>
<li>What is the top speed the probe has reached so far?</li>
</ul>
</li>
<li>Since the measurement data from the NASA and ESA probes are processed by different teams, they should be able to be consumed separately.<ul>
<li>Data from the ESA probes are to be converted from the imperial to the metric system.</li>
</ul>
</li>
</ul>
<h2 id="The-target-architecture"><a href="#The-target-architecture" class="headerlink" title="The target architecture"></a>The target architecture</h2><p>In our example, we are dealing with a continuous stream of readings that we consider our events. Since we have to perform a series of transformations and aggregations on these, the use case lends itself well to processing as a stream. The need to aggregate measurement data also suggests that some part of our application needs to be able to remember a state in order to keep the summed values per probe.</p>
<p>To implement the use case, we divide the application into 3 subcomponents. A Kafka cluster forms the central hub for communication between the components:</p>
<p><img src="/images/Xy3kzvYgT6GRsaxYg6va.png" srcset="/images/loading.gif" lazyload alt="Architectural sketch of our sample application"><br>We will build this example architecture to illustrate our use case. We use Kafka as the central communication hub between our services.</p>
<p>We arrange the distribution of tasks between the services as follows:</p>
<ul>
<li><strong>kafka-samples-producer</strong> : Converts the received measurement data into a machine-readable format and stores it on a Kafka topic. Since we don’t have any real space probes handy at the moment, we let this service generate random measurement data.</li>
<li><strong>kafka-samples-streams</strong> : Performs the calculation of the aggregated measurement data and the subdivision by measurement data for NASA or ESA. Since the previously calculated values are also included in the calculation, the application must maintain a local state. We map this using the Streams API in the form of two KTables (we already separate them by space agency here). The KTables are materialized transparently for the application by a so-called state store, which saves the history of the state in Kafka Topics.</li>
<li><strong>kafka-samples-consumer</strong> : Represents an example client service of a space agency, which is responsible for the further processing of the aggregated measurement data. In our case, this reads both output topics, in the case of the ESA, converts them to the metric system and logs these values to stdout.</li>
</ul>
<h2 id="Implementation-of-the-Services"><a href="#Implementation-of-the-Services" class="headerlink" title="Implementation of the Services"></a>Implementation of the Services</h2><p>We have implemented all services with Spring Boot and Kotlin and use the for configuration and implementation<a target="_blank" rel="noopener" href="https://spring.io/projects/spring-cloud-stream">Spring abstraction for streams</a>. In the following sections we will go into the concrete implementation of the individual services.</p>
<h3 id="Generation-of-the-telemetry-data-kafka-samples-producer"><a href="#Generation-of-the-telemetry-data-kafka-samples-producer" class="headerlink" title="Generation of the telemetry data (kafka-samples-producer)"></a>Generation of the telemetry data (kafka-samples-producer)</h3><p>To write the (fictitious) probe measurement data, we use the Kafka Producer API, available in Spring via the<a target="_blank" rel="noopener" href="https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#_apache_kafka_binder">Spring Cloud Stream Binder for Kafka</a> provided. We configure the service via the (<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-producer/src/main/resources/application.yml">application.yml</a>) as follows:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">application:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">kafka-telemetry-data-producer</span><br>  <span class="hljs-attr">cloud:</span><br>    <span class="hljs-attr">stream:</span><br>      <span class="hljs-attr">kafka:</span><br>        <span class="hljs-attr">binder:</span><br>          <span class="hljs-attr">brokers:</span> <span class="hljs-string">&quot;localhost:29092&quot;</span><br>        <span class="hljs-attr">bindings:</span><br>          <span class="hljs-attr">telemetry-data-out-0:</span><br>            <span class="hljs-attr">producer:</span><br>              <span class="hljs-attr">configuration:</span><br>                <span class="hljs-attr">key.serializer:</span> <span class="hljs-string">org.springframework.kafka.support.serializer.ToStringSerializer</span><br>                <span class="hljs-attr">value.serializer:</span> <span class="hljs-string">org.springframework.kafka.support.serializer.JsonSerializer</span><br>                <span class="hljs-comment"># Otherwise com.metao.samples.kafkasamplesproducer.event.TelemetryData will be added as a header info</span><br>                <span class="hljs-comment"># which can&#x27;t be deserialized by consumers (unless they have kafka.properties.spring.json.use.type.headers: false themselves)</span><br>                <span class="hljs-attr">spring.json.add.type.headers:</span> <span class="hljs-literal">false</span><br>      <span class="hljs-attr">bindings:</span><br>        <span class="hljs-attr">telemetry-data-out-0:</span><br>          <span class="hljs-attr">producer:</span><br>            <span class="hljs-comment"># use kafka internal encoding</span><br>            <span class="hljs-attr">useNativeEncoding:</span> <span class="hljs-literal">true</span><br>          <span class="hljs-attr">destination:</span> <span class="hljs-string">space-probe-telemetry-data</span><br></code></pre></td></tr></table></figure>

<p>The configuration consists of a Kafka-specific (upper <code>bindings</code>configuration block) and a technology-agnostic (lower <code>bindings</code>configuration block) part, which are bound together via the binding.</p>
<p>In the example we create the binding <code>telemetry-data-out-0</code>. This declaration is based on the following convention:</p>
<figure class="highlight bnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bnf"><span class="hljs-attribute">&lt;Funktionsname&gt;</span>-<span class="hljs-attribute">&lt;in|out&gt;</span>-<span class="hljs-attribute">&lt;n&gt;</span><br></code></pre></td></tr></table></figure>

<p>The <code>in</code>or <code>out</code>defines whether the binding is an input (an incoming stream of data) or an output (an outgoing stream of data). With the number increasing from 0 at the end, a function can be attached to several bindings - and thus read from several topics with one function - or written to several topics.</p>
<p>In the Kafka-specific part, we prevent Spring from adding a Type header to every message. Otherwise, this would mean that a consumer of the message - should he not actively prevent this - does not know the class specified in the header and therefore cannot deserialize the message.</p>
<p>The technology-agnostic part is used in this form for all Spring Cloud Streams-supported implementations like RabbitMQ, AWS SQS, etc. <code>destination</code>All you have to do here is specify the output target ( ) – in our case, this maps to the name of the Kafka topic that we want to describe.</p>
<p>After the service is configured, we define a Spring component to write the metrics (<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-producer/src/main/kotlin/com/metao1/samples/kafkasamplesproducer/TelemetryDataStreamBridge.kt">TelemetryDataStreamBridge.kt</a>):</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs stylus">package com<span class="hljs-selector-class">.metao</span><span class="hljs-selector-class">.samples</span><span class="hljs-selector-class">.kafkasamplesproducer</span><br><br>import com<span class="hljs-selector-class">.metao</span><span class="hljs-selector-class">.samples</span><span class="hljs-selector-class">.kafkasamplesproducer</span><span class="hljs-selector-class">.event</span><span class="hljs-selector-class">.TelemetryData</span><br>import mu<span class="hljs-selector-class">.KotlinLogging</span><br>import org<span class="hljs-selector-class">.springframework</span><span class="hljs-selector-class">.beans</span><span class="hljs-selector-class">.factory</span><span class="hljs-selector-class">.annotation</span><span class="hljs-selector-class">.Autowired</span><br>import org<span class="hljs-selector-class">.springframework</span><span class="hljs-selector-class">.cloud</span><span class="hljs-selector-class">.stream</span><span class="hljs-selector-class">.function</span><span class="hljs-selector-class">.StreamBridge</span><br>import org<span class="hljs-selector-class">.springframework</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.support</span><span class="hljs-selector-class">.KafkaHeaders</span><br>import org<span class="hljs-selector-class">.springframework</span><span class="hljs-selector-class">.messaging</span><span class="hljs-selector-class">.support</span><span class="hljs-selector-class">.MessageBuilder</span><br>import org<span class="hljs-selector-class">.springframework</span><span class="hljs-selector-class">.stereotype</span><span class="hljs-selector-class">.Component</span><br><br>@Component<br>class <span class="hljs-built_in">TelemetryDataStreamBridge</span>(@Autowired val streamBridge: StreamBridge) &#123;<br><br>    private val logger = KotlinLogging<span class="hljs-selector-class">.logger</span> &#123;&#125;<br><br>    fun <span class="hljs-built_in">send</span>(telemetryData: TelemetryData) &#123;<br>        val kafkaMessage = MessageBuilder<br>            <span class="hljs-selector-class">.withPayload</span>(telemetryData)<br>            <span class="hljs-comment">// Make sure all messages for a given probe go to the same partition to ensure proper ordering</span><br>            <span class="hljs-selector-class">.setHeader</span>(KafkaHeaders<span class="hljs-selector-class">.MESSAGE_KEY</span>, telemetryData.probeId)<br>            <span class="hljs-selector-class">.build</span>()<br>        logger<span class="hljs-selector-class">.info</span> &#123; <span class="hljs-string">&quot;Publishing space probe telemetry data: Payload: &#x27;$&#123;kafkaMessage.payload&#125;&#x27;&quot;</span> &#125;<br>        streamBridge<span class="hljs-selector-class">.send</span>(<span class="hljs-string">&quot;telemetry-data-out-0&quot;</span>, kafkaMessage)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>As an entry point into the streaming world, Spring Cloud Streaming offers two different options:</p>
<ul>
<li>The imperative<code>StreamBridge</code></li>
<li>the reactive one<code>EmitterProcessor</code></li>
</ul>
<p>For this use case we use the <code>StreamBridge</code>. We can have Spring inject this and write the generated probe data to our topic. We use the ID of the respective probe as the message key, so that data from a probe always end up on the same partition. <code>send()</code>We pass the binding created in the configuration to the function .</p>
<h3 id="Processing-of-the-telemetry-data-kafka-samples-streams"><a href="#Processing-of-the-telemetry-data-kafka-samples-streams" class="headerlink" title="Processing of the telemetry data (kafka-samples-streams)"></a>Processing of the telemetry data (kafka-samples-streams)</h3><p>Most of the use case is processed in this part of our application. We use the Kafka Streams API to consume the generated probe data, perform the necessary calculations, and then write the aggregated measurement data to the two target topics. In Spring Boot, we can access the Streams API via the<a target="_blank" rel="noopener" href="https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#_kafka_streams_binder">Spring Cloud Stream Binder for Kafka Streams</a>.</p>
<p>Analogous to the Producer API, we start with creating our bindings and configure our service via the file <a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-streams/src/main/resources/application.yml">application.yml</a>.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs stylus">spring:<br>  kafka<span class="hljs-selector-class">.properties</span><span class="hljs-selector-class">.spring</span><span class="hljs-selector-class">.json</span><span class="hljs-selector-class">.use</span><span class="hljs-selector-class">.type</span><span class="hljs-selector-class">.headers</span>: false<br>  application:<br>    name: kafka-telemetry-data-aggregator<br>  cloud:<br>    function:<br>      definition: aggregateTelemetryData<br>    stream:<br>      bindings:<br>        aggregateTelemetryData-<span class="hljs-keyword">in</span>-<span class="hljs-number">0</span>:<br>          destination: space-probe-telemetry-data<br>        aggregateTelemetryData-out-<span class="hljs-number">0</span>:<br>          destination: space-probe-aggregate-telemetry-data-nasa<br>        aggregateTelemetryData-out-<span class="hljs-number">1</span>:<br>          destination: space-probe-aggregate-telemetry-data-esa<br>      kafka:<br>        binder:<br>          brokers: <span class="hljs-string">&quot;localhost:29092&quot;</span><br>        streams:<br>          bindings:<br>            aggregateTelemetryData-<span class="hljs-keyword">in</span>-<span class="hljs-number">0</span><span class="hljs-selector-class">.consumer</span>:<br>              keySerde: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.common</span><span class="hljs-selector-class">.serialization</span>.Serdes<span class="hljs-variable">$StringSerde</span><br>              valueSerde: com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><span class="hljs-selector-class">.serdes</span><span class="hljs-selector-class">.TelemetryDataPointSerde</span><br>              deserializationExceptionHandler: logAndContinue<br>            aggregateTelemetryData-out-<span class="hljs-number">0</span><span class="hljs-selector-class">.producer</span>:<br>              keySerde: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.common</span><span class="hljs-selector-class">.serialization</span>.Serdes<span class="hljs-variable">$StringSerde</span><br>              valueSerde: com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><span class="hljs-selector-class">.serdes</span><span class="hljs-selector-class">.AggregateTelemetryDataSerde</span><br>            aggregateTelemetryData-out-<span class="hljs-number">1</span><span class="hljs-selector-class">.producer</span>:<br>              keySerde: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.common</span><span class="hljs-selector-class">.serialization</span>.Serdes<span class="hljs-variable">$StringSerde</span><br>              valueSerde: com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><span class="hljs-selector-class">.serdes</span><span class="hljs-selector-class">.AggregateTelemetryDataSerde</span><br>management:<br>  endpoints:<br>    web:<br>      exposure:<br>        include: <span class="hljs-string">&quot;*&quot;</span><br></code></pre></td></tr></table></figure>

<p>To implement the feature, we use the <strong><em>functional style</em></strong> that was introduced with Spring Cloud Stream 3.0.0. To do this, we specify <code>aggregateTelemetryData</code> the name of our bean in the <em>function definition</em> that implements the function. This will contain the actual technical logic.</p>
<p>Since we are reading from one topic and writing to two topics, we need three bindings here:</p>
<ul>
<li>A <code>IN</code>binding to consume our metrics</li>
<li>A <code>OUT</code>binding to write our aggregated measurement data for NASA</li>
<li>A <code>OUT</code>binding to write our aggregated measurement data for the ESA</li>
</ul>
<p><em>We can view the function</em> declared in the upper part of the configuration as a mapping of our <code>IN</code>bindings to our <code>OUT</code>bindings. In order for this to be associated with the bindings, we must adhere to the Spring convention described in the previous section.</p>
<p>With the binding configuration complete, we can move on to implementing our business logic. To do this, we create a function that matches the name of the functional binding from our configuration. This function maps our Kafka Streams topology and calculation logic (<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-streams/src/main/kotlin/com/example/kafkasamplesstreams/KafkaStreamsHandler.kt">KafkaStreamsHandler.kt</a>):</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs stylus">package com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><br><br>import com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><span class="hljs-selector-class">.events</span><span class="hljs-selector-class">.AggregatedTelemetryData</span><br>import com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><span class="hljs-selector-class">.events</span><span class="hljs-selector-class">.SpaceAgency</span><br>import com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><span class="hljs-selector-class">.events</span><span class="hljs-selector-class">.TelemetryDataPoint</span><br>import com<span class="hljs-selector-class">.example</span><span class="hljs-selector-class">.kafkasamplesstreams</span><span class="hljs-selector-class">.serdes</span><span class="hljs-selector-class">.AggregateTelemetryDataSerde</span><br>import mu<span class="hljs-selector-class">.KotlinLogging</span><br>import org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.common</span><span class="hljs-selector-class">.serialization</span><span class="hljs-selector-class">.Serdes</span><br>import org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.streams</span><span class="hljs-selector-class">.kstream</span><span class="hljs-selector-class">.KStream</span><br>import org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.streams</span><span class="hljs-selector-class">.kstream</span><span class="hljs-selector-class">.Materialized</span><br>import org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.kafka</span><span class="hljs-selector-class">.streams</span><span class="hljs-selector-class">.kstream</span><span class="hljs-selector-class">.Predicate</span><br>import org<span class="hljs-selector-class">.springframework</span><span class="hljs-selector-class">.context</span><span class="hljs-selector-class">.annotation</span><span class="hljs-selector-class">.Bean</span><br>import org<span class="hljs-selector-class">.springframework</span><span class="hljs-selector-class">.context</span><span class="hljs-selector-class">.annotation</span><span class="hljs-selector-class">.Configuration</span><br><br><br>@Configuration<br>class KafkaStreamsHandler &#123;<br><br>    private val logger = KotlinLogging<span class="hljs-selector-class">.logger</span> &#123;&#125;<br><br>    @Bean<br>    fun <span class="hljs-built_in">aggregateTelemetryData</span>(): java<span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.function</span>.Function&lt;<br>            KStream&lt;String, TelemetryDataPoint&gt;,<br>            Array&lt;KStream&lt;String, AggregatedTelemetryData&gt;&gt;&gt; &#123;<br>        return java<span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.function</span>.Function&lt;<br>                KStream&lt;String, TelemetryDataPoint&gt;,<br>                Array&lt;KStream&lt;String, AggregatedTelemetryData&gt;&gt;&gt; &#123; telemetryRecords -&gt;<br>            telemetryRecords<span class="hljs-selector-class">.branch</span>(<br>                <span class="hljs-comment">// Split up the processing pipeline into 2 streams, depending on the space agency of the probe</span><br>                Predicate &#123; _, v -&gt; v<span class="hljs-selector-class">.spaceAgency</span> == SpaceAgency<span class="hljs-selector-class">.NASA</span> &#125;,<br>                Predicate &#123; _, v -&gt; v<span class="hljs-selector-class">.spaceAgency</span> == SpaceAgency<span class="hljs-selector-class">.ESA</span> &#125;<br>            )<span class="hljs-selector-class">.map</span> &#123; telemetryRecordsPerAgency -&gt;<br>                <span class="hljs-comment">// Apply aggregation logic on each stream separately</span><br>                telemetryRecordsPerAgency<br>                    <span class="hljs-selector-class">.groupByKey</span>()<br>                    <span class="hljs-selector-class">.aggregate</span>(<br>                        <span class="hljs-comment">// KTable initializer</span><br>                        &#123; <span class="hljs-built_in">AggregatedTelemetryData</span>(maxSpeedMph = <span class="hljs-number">0.0</span>, traveledDistanceFeet = <span class="hljs-number">0.0</span>) &#125;,<br>                        <span class="hljs-comment">// Calculation function for telemetry data aggregation</span><br>                        &#123; probeId, lastTelemetryReading, aggregatedTelemetryData -&gt;<br>                            <span class="hljs-built_in">updateTotals</span>(<br>                                probeId,<br>                                lastTelemetryReading,<br>                                aggregatedTelemetryData<br>                            )<br>                        &#125;,<br>                        <span class="hljs-comment">// Configure Serdes for State Store topic</span><br>                        Materialized<span class="hljs-selector-class">.with</span>(Serdes<span class="hljs-selector-class">.StringSerde</span>(), <span class="hljs-built_in">AggregateTelemetryDataSerde</span>())<br>                    )<br>                    <span class="hljs-selector-class">.toStream</span>()<br>            &#125;<span class="hljs-selector-class">.toTypedArray</span>()<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * Performs calculation of per-probe aggregate measurement data.</span><br><span class="hljs-comment">     * The currently calculated totals are held in a Kafka State Store</span><br><span class="hljs-comment">     * backing the KTable created with aggregate() and the most recently</span><br><span class="hljs-comment">     * created aggregate telemetry data record is passed on downstream.</span><br><span class="hljs-comment">     */</span><br>    fun <span class="hljs-built_in">updateTotals</span>(<br>        probeId: String,<br>        lastTelemetryReading: TelemetryDataPoint,<br>        currentAggregatedValue: AggregatedTelemetryData<br>    ): AggregatedTelemetryData &#123;<br>        val totalDistanceTraveled =<br>            lastTelemetryReading<span class="hljs-selector-class">.traveledDistanceFeet</span> + currentAggregatedValue<span class="hljs-selector-class">.traveledDistanceFeet</span><br>        val maxSpeed = <span class="hljs-keyword">if</span> (lastTelemetryReading<span class="hljs-selector-class">.currentSpeedMph</span> &gt; currentAggregatedValue.maxSpeedMph)<br>            lastTelemetryReading<span class="hljs-selector-class">.currentSpeedMph</span> <span class="hljs-keyword">else</span> currentAggregatedValue<span class="hljs-selector-class">.maxSpeedMph</span><br>        val aggregatedTelemetryData = <span class="hljs-built_in">AggregatedTelemetryData</span>(<br>            traveledDistanceFeet = totalDistanceTraveled,<br>            maxSpeedMph = maxSpeed<br>        )<br>        logger<span class="hljs-selector-class">.info</span> &#123;<br>            <span class="hljs-string">&quot;Calculated new aggregated telemetry data for probe $probeId. New max speed: $&#123;aggregatedTelemetryData.maxSpeedMph&#125; and &quot;</span> +<br>                    <span class="hljs-string">&quot;traveled distance $&#123;aggregatedTelemetryData.traveledDistanceFeet&#125;&quot;</span><br><br>        &#125;<br>        return aggregatedTelemetryData<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>In order to calculate the aggregated telemetry data per probe, we have to solve three problems in the implementation of the function:</p>
<h4 id="Implementation-of-the-calculation"><a href="#Implementation-of-the-calculation" class="headerlink" title="Implementation of the calculation"></a>Implementation of the calculation</h4><p>The Kafka Streams API provides a set of predefined operations that we can apply to the incoming stream to perform the computation according to the use case. Since we need to separate the aggregated probe data by space agency, we first use the operation, which returns <code>branch()</code>an array of as a result . <code>KStream</code>We get two streams, which are now already separated by space agency. The probe data can now be calculated. Since the calculation is identical for both agencies, we use <code>map()</code>Kotlin’s operation so that we only have to define the following steps once for both streams. To group and ultimately aggregate the probe data by their Probe ID, we again use native Stream API operations. The operation<a target="_blank" rel="noopener" href="https://docs.confluent.io/platform/current/streams/concepts.html#aggregations"><code>aggregate()</code></a> needs three parameters:</p>
<ul>
<li>An <strong>Initializer</strong> that determines the initial value if there is no aggregated data yet</li>
<li>An <strong>Aggregator</strong> function that determines our calculation logic for aggregating the metrics</li>
<li>The <strong>serializers&#x2F;deserializers</strong> to use to store the aggregated values</li>
</ul>
<p><code>aggregate()</code>As a result, the operation returns one that <code>KTable</code>contains the most recently calculated total value for each probe ID. Since our customers are interested in the most up-to-date data, we convert it <code>KTable</code>back into one <code>KStream</code>- every change in the <code>KTable</code>generates an event that contains the last calculated total value.</p>
<h4 id="Storage-of-the-aggregated-measurement-data"><a href="#Storage-of-the-aggregated-measurement-data" class="headerlink" title="Storage of the aggregated measurement data"></a>Storage of the aggregated measurement data</h4><p>The <code>aggregate()</code>function that we use in our example to calculate the total values is a so-called <em>stateful operation</em> - i.e. a stateful operation that requires a local state in order to be able to take into account all previously calculated values for calculating the currently valid total value. The Kafka Streams API handles the management of state for us by <code>KTable</code>materializing the operation a . One <code>KTable</code>can be thought of as a changelog for key&#x2F;value pairs, which allows us to persist (and restore if necessary) a local state. <code>KTables</code>are in the Kafka cluster by so-called _state stores_materialized. The state store uses a topic managed by Kafka to persist data in the cluster. This saves us from having to manage other infrastructure components, such as a database to keep track of the status.</p>
<h4 id="Delivery-to-the-various-space-agencies"><a href="#Delivery-to-the-various-space-agencies" class="headerlink" title="Delivery to the various space agencies"></a>Delivery to the various space agencies</h4><p>In order to supply NASA and ESA with the aggregated measurement data relevant to them, we used the operation before the calculation <code>branch()</code>. As a result, our function has a return value of <code>Array&gt;</code>, whose indices correlate with the space agencies. In our case, this means that <code>Array[0]</code>the data is from NASA and <code>Array[1]</code>the data from ESA. This division in turn matches our binding config.</p>
<p>The resulting KStreams are our aggregation result and are written to the two output topics.</p>
<h3 id="Consume-the-telemetry-data-kafka-samples-consumer"><a href="#Consume-the-telemetry-data-kafka-samples-consumer" class="headerlink" title="Consume the telemetry data (kafka-samples-consumer)"></a>Consume the telemetry data (kafka-samples-consumer)</h3><p>To read the aggregated probe measurement data, we use the Kafka Consumer API, which, like the Producer, is available via the<a target="_blank" rel="noopener" href="https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#_apache_kafka_binder">Spring Cloud Stream Binder for Kafka</a> provided. We configure the service for this as follows (<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-consumer/src/main/resources/application.yml">application.yml</a>):</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">application:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">kafka-telemetry-data-consumer</span><br>  <span class="hljs-comment"># Ignore type headers in kafka message</span><br>  <span class="hljs-attr">kafka.properties.spring.json.use.type.headers:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">cloud:</span><br>    <span class="hljs-attr">stream:</span><br>      <span class="hljs-attr">kafka:</span><br>        <span class="hljs-attr">binder:</span><br>          <span class="hljs-attr">brokers:</span> <span class="hljs-string">&quot;localhost:29092&quot;</span><br>        <span class="hljs-attr">bindings:</span><br>          <span class="hljs-comment"># this has to match the consumer bean name with suffix in-0 (for consumer)</span><br>          <span class="hljs-attr">processNasaTelemetryData-in-0:</span><br>            <span class="hljs-attr">consumer:</span><br>              <span class="hljs-attr">configuration:</span><br>                <span class="hljs-attr">key.deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>                <span class="hljs-attr">value.deserializer:</span> <span class="hljs-string">com.metao.samples.kafkasamplesconsumer.serdes.TelemetryDataDeserializer</span><br>          <span class="hljs-attr">processEsaTelemetryData-in-0:</span><br>            <span class="hljs-attr">consumer:</span><br>              <span class="hljs-attr">configuration:</span><br>                <span class="hljs-attr">key.deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>                <span class="hljs-attr">value.deserializer:</span> <span class="hljs-string">com.metao.samples.kafkasamplesconsumer.serdes.TelemetryDataDeserializer</span><br>      <span class="hljs-attr">bindings:</span><br>        <span class="hljs-attr">processNasaTelemetryData-in-0:</span><br>          <span class="hljs-attr">group:</span> <span class="hljs-string">$&#123;spring.application.name&#125;</span><br>          <span class="hljs-attr">destination:</span> <span class="hljs-string">space-probe-aggregate-telemetry-data-nasa</span><br>        <span class="hljs-attr">processEsaTelemetryData-in-0:</span><br>          <span class="hljs-attr">group:</span> <span class="hljs-string">$&#123;spring.application.name&#125;</span><br>          <span class="hljs-attr">destination:</span> <span class="hljs-string">space-probe-aggregate-telemetry-data-esa</span><br>    <span class="hljs-attr">function:</span><br>      <span class="hljs-comment"># We define this explicitly since we have several consumer functions</span><br>      <span class="hljs-attr">definition:</span> <span class="hljs-string">processNasaTelemetryData;processEsaTelemetryData</span><br></code></pre></td></tr></table></figure>

<p>We continue according to the familiar pattern: We implement beans that implement the binding, starting with NASA.</p>
<p>We create a function that <code>Consumer</code>implements (see<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-consumer/src/main/kotlin/com/metao/samples/kafkasamplesconsumer/KafkaConsumerConfiguration.kt">KafkaConsumerConfiguration.kt</a>). This means that we have an input but no output to another topic and the stream ends in this function.</p>
<p>The consumer for ESA follows the same pattern as the consumer for NASA, the only difference being that the data transmitted is converted from the imperial system to the metric system. We have this functionality in the <code>init</code>function of our class<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-consumer/src/main/kotlin/com/metao/samples/kafkasamplesconsumer/event/MetricTelemetryData.kt">MetricTelemetryData.kt</a>  capsuled.</p>
<p>With the implementation of the consumer, our stream processing pipeline is complete and all requirements have been implemented.</p>
<h2 id="The-finished-solution-in-action"><a href="#The-finished-solution-in-action" class="headerlink" title="The finished solution in action"></a>The finished solution in action</h2><p>If we start our services now, after a few seconds we should see the first aggregated probe data in the Consumer Service log. Additionally, we can take a look at<a target="_blank" rel="noopener" href="https://github.com/tchiotludo/akhq">AKHQ</a> Get an overview of the topics and messages in Kafka:</p>
<p><img src="/images/K3ygZt4RvWW2Evm17u2w.png" srcset="/images/loading.gif" lazyload alt="AKHQ Web UI in Kafka Topics view"><br>We recognize the inbound and outbound topics accessed by our services, as well as the state stores that our aggregator service has materialized behind the scenes for us in the form of Kafka topics.</p>
<h2 id="Lessons-learned"><a href="#Lessons-learned" class="headerlink" title="Lessons learned"></a>Lessons learned</h2><p>If you are now thinking about using the whole thing in your projects, we have prepared some questions and argumentation aids for you so that you can make the right decision for you.</p>
<h3 id="When-should-you-think-about-using-stream-processing"><a href="#When-should-you-think-about-using-stream-processing" class="headerlink" title="When should you think about using stream processing?"></a>When should you think about using stream processing?</h3><p>Stream processing can be useful wherever you are faced with processing large amounts of data and time delays need to be minimized. The decision for or against stream processing should not be based on a single component - the solution should fit the overall architecture of the system and the problem. If your use case has the following attributes, stream processing could be a solution:</p>
<ul>
<li>You are faced with a constant stream of data. Example: IoT devices continuously send you sensor data.</li>
<li>Your workload is continuous and does not have the character of a data delivery. Example: You receive a data export from an old system once a day and must be able to definitely determine the end of a delivery, for example. Batch processing usually makes more sense here.</li>
<li>The data to be processed are time-critical and must be processed immediately. In the case of larger amounts of data or complex calculations, you always have to think about the scalability of your services in order to keep the processing time low. Streams are suitable for this because you can scale horizontally quite easily through partitioning and asynchronous event processing.</li>
</ul>
<h3 id="Should-you-use-Kafka-for-your-stream-processing-workloads"><a href="#Should-you-use-Kafka-for-your-stream-processing-workloads" class="headerlink" title="Should you use Kafka for your stream processing workloads?"></a>Should you use Kafka for your stream processing workloads?</h3><p>We give a cautious <em>“yes”</em> to that . Kafka’s data storage, paired with the Stream Processing API, is a powerful tool and is offered by various providers <em>as a service</em> . This flattens the learning curve and minimizes maintenance. In event-driven use cases, this feels good and right. Unfortunately, we have seen several times that Kafka is used in application architectures as a pure message bus, for batch workloads or in situations where synchronous communication between services would have made more sense. The advantages of Kafka and event-driven architectures remain unused or worse: we find it more difficult than necessary to solve the problem.</p>
<p>If Kafka is already present in your architecture and your problem fits the technology, we would recommend you to start with the<a target="_blank" rel="noopener" href="https://docs.confluent.io/platform/current/streams/concepts.html#stream-processing-application">Stream API capabilities</a> to deal with - data pipelines can often be set up without additional infrastructure components and you can do without components such as relational databases or in-memory data stores. Confluent offers very good<a target="_blank" rel="noopener" href="https://www.confluent.io/blog/how-kafka-streams-works-guide-to-stream-processing/">Documents to get you started</a> an.</p>
<p>In cases where you cannot use Kafka <em>as a service</em> , the effort involved in setting up a Kafka cluster and operating it yourself can outweigh the benefits. In these cases, it may therefore make more sense to use a classic message broker and a relational database.</p>
<h3 id="Should-you-implement-stream-processing-workloads-with-Kafka-Streams-and-Spring-Boot"><a href="#Should-you-implement-stream-processing-workloads-with-Kafka-Streams-and-Spring-Boot" class="headerlink" title="Should you implement stream processing workloads with Kafka Streams and Spring Boot?"></a>Should you implement stream processing workloads with Kafka Streams and Spring Boot?</h3><p>Clear answer: It depends. If you are already using Spring Boot across the board in your projects, the<a target="_blank" rel="noopener" href="https://spring.io/projects/spring-cloud-stream">Spring Streams abstraction</a> save some time when commissioning new services, since configuration and implementation always follow a very similar scheme and we can hide some of the complexity during implementation. However, the Spring path is not quite perfect. Here are the issues that caused us pain:</p>
<ul>
<li><strong>Conventions &amp; Documentation</strong> : The configuration with the Spring abstraction consists of a few conventions that are not always properly documented and are sometimes non-transparent, which can cost nerves and time. At the time of writing this article, parts of the Spring documentation were out of date (e.g. the functional programming paradigm we are using is not yet mentioned in the current version of the documentation)</li>
<li><strong>Error Handling</strong> : When using the Stream Binder for Kafka Streams as in our class<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-streams/src/main/kotlin/com/example/kafkasamplesstreams/KafkaStreamsHandler.kt">KafkaStreamsHandler.kt</a> There is currently no convenient solution for handling exceptions that occur outside of deserialization using on-board tools (we define what should happen to errors during deserialization in<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/kafka-samples-streams/src/main/resources/application.yml#L24">application.yml</a>). The only solution for this at the moment is<a target="_blank" rel="noopener" href="https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#_handling_non_deserialization_exceptions">to implement error handling past the Streams API</a> or ensure that any deserialization errors are caught. Provides an exemplary approach<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/main/src/main/kotlin/com/example/kafkasamplesstreams/TelemetryAggregationTransformer.kt">TelemetryAggregationTransformer.kt</a>. By bypassing the Streams API, we can handle errors at the message level, for example by <code>try/catch</code>implementing logic. Since we have descended an abstraction level in this example, we unfortunately also lose the automatic state management <code>KTables</code>- we have to manage state stores ourselves if necessary. In this case, unfortunately, you currently have to weigh up what is more important to you.</li>
<li><strong>Up-to- dateness</strong> : The Spring Dependencies are always a few Kafka releases behind, so that not all features can always be used immediately (see previous point).</li>
</ul>
<p>As an alternative to the Spring abstraction, there are various freely usable libraries to integrate the concepts from Kafka Streams into various tech stacks. Confluent offers<a target="_blank" rel="noopener" href="https://developer.confluent.io/kafka-languages-and-tools/">well-documented step-by-step recipes</a> for a wide range of supported environments and programming languages to keep the barriers to entry low, regardless of your environment. In this respect, you are free to decide here. If you feel comfortable with Spring Boot: great! If not: that’s ok too!</p>
<h2 id="A-few-final-words"><a href="#A-few-final-words" class="headerlink" title="A few final words"></a>A few final words</h2><p>In this blog post, we have demonstrated how you can implement the concepts of stream processing using a concrete use case with Spring Boot and Kafka Streams. We hope that with Stream Processing you now have another tool in your toolbox and that you can now approach your next project with complete peace of mind.</p>
<p>You can find the complete code for our sample project at<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry">GitHub</a>.</p>
<h2 id="bonus-material"><a href="#bonus-material" class="headerlink" title="bonus material"></a>bonus material</h2><p>In order not to go beyond the scope of our blog post, we have limited ourselves to a fairly simple use case. However <code>KTables</code>, much more demanding scenarios can also be implemented with . Another slightly more complex example (how do we merge multiple incoming streams?) can be found in our GitHub repo on a<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/tree/feature/demonstrate-ktable-joins">separate branch</a>. We combine the incoming streams in the class<a target="_blank" rel="noopener" href="https://github.com/metao1/spring-kafka-streams-telemetry/blob/feature/demonstrate-ktable-joins/kafka-samples-streams/src/main/kotlin/com/example/kafkasamplesstreams/KafkaStreamsHandler.kt">KafkaStreamsHandler.kt</a> using the <code>join()</code>operation.</p>
<h2 id="credentials"><a href="#credentials" class="headerlink" title="credentials"></a>credentials</h2><p>[1] Ben Stopford (2018):<a target="_blank" rel="noopener" href="https://www.confluent.io/designing-event-driven-systems/">Designing Event Driven Systems</a> , S. 29 ff.</p>
<p>[2] Martin Kleppmann (2015): <a target="_blank" rel="noopener" href="https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/">Turning the Database inside-out with Apache Samza</a></p>
<p>[3] Apache Software Foundation (2017): <a target="_blank" rel="noopener" href="https://kafka.apache.org/30/documentation/streams/core-concepts">Kafka Streams Core Concepts</a></p>
<p>[4] Apache Software Foundation (2017): <a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/#intro_concepts_and_terms">Kafka Main Concepts and Terminology</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Spring Boot and Kafka Stream, Processing continuous data streams</div>
      <div>https://java-springboot-kafka.github.io/2022/11/13/kafka-streams-and-spring-boot/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Mehrdad Karami</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>November 13, 2022</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/11/13/what_is_new_in_java_17/" title="What&#39;s new in Java 17">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">What&#39;s new in Java 17</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/11/13/What_is_new_in_java_19/" title="What&#39;s new in Java 19">
                        <span class="hidden-mobile">What&#39;s new in Java 19</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    

  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Views: 
        <span id="busuanzi_value_site_pv"></span>
        
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Visitors: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
